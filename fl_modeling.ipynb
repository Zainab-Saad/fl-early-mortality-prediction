{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 11:04:57.619088: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-21 11:04:57.622191: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-21 11:04:57.667251: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-21 11:04:58.369236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/tmp/ipykernel_47409/859392504.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  vitals = pd.read_sql_query(vitals_query, conn)\n",
      "/tmp/ipykernel_47409/859392504.py:23: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  labs = pd.read_sql_query(labs_query, conn)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Constants\n",
    "WINDOW_LENGTH = 24  # Adjust as needed\n",
    "MIN_LOS_ICU = 48    # Adjust as needed\n",
    "MAX_TIMESTEPS = 10  # Set an upper limit for the number of timesteps\n",
    "CLIENT_COUNT = 4    # Number of clients\n",
    "\n",
    "# Connect to db\n",
    "conn = psycopg2.connect(host='localhost', port=5432, dbname='mimic', user='zainab', password='password')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Read vital signs\n",
    "vitals_query = f'SELECT * FROM mimiciii.vitals_windowed_{WINDOW_LENGTH}h;'\n",
    "vitals = pd.read_sql_query(vitals_query, conn)\n",
    "\n",
    "# Read in labs values\n",
    "labs_query = f'SELECT * FROM mimiciii.labs_windowed_{WINDOW_LENGTH}h;'\n",
    "labs = pd.read_sql_query(labs_query, conn)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Convert datetime columns to seconds\n",
    "for col in vitals.select_dtypes(include=['datetime64', 'timedelta64']).columns:\n",
    "    vitals[col] = vitals[col].astype(int) / 10**9  # Convert to seconds\n",
    "\n",
    "for col in labs.select_dtypes(include=['datetime64', 'timedelta64']).columns:\n",
    "    labs[col] = labs[col].astype(int) / 10**9  # Convert to seconds\n",
    "\n",
    "# Convert categorical columns to numeric using one-hot encoding\n",
    "vitals = pd.get_dummies(vitals, drop_first=True)\n",
    "labs = pd.get_dummies(labs, drop_first=True)\n",
    "\n",
    "# Merge the vitals and labs data on the common key (icustay_id)\n",
    "merged_data = pd.merge(vitals, labs, on='icustay_id', suffixes=('_vitals', '_labs'), how='inner')\n",
    "\n",
    "# Ensure the labels are the same in both datasets\n",
    "assert all(merged_data['label_death_icu_vitals'] == merged_data['label_death_icu_labs']), \"Mismatch in labels between vitals and labs\"\n",
    "\n",
    "# Drop one of the label columns\n",
    "merged_data = merged_data.drop(columns=['label_death_icu_labs'])\n",
    "\n",
    "# Rename the remaining label column for consistency\n",
    "merged_data = merged_data.rename(columns={'label_death_icu_vitals': 'label_death_icu'})\n",
    "\n",
    "vitals_features = merged_data.filter(like='_vitals')\n",
    "labs_features = merged_data.filter(like='_labs')\n",
    "labels = merged_data['label_death_icu']\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_vitals = vitals_features.to_numpy()\n",
    "X_labs = labs_features.to_numpy()\n",
    "y = labels.to_numpy()\n",
    "\n",
    "# Calculate the number of timesteps that divides the total features without a remainder\n",
    "def find_timesteps(total_features, max_timesteps):\n",
    "    for timesteps in range(max_timesteps, 0, -1):\n",
    "        if total_features % timesteps == 0:\n",
    "            return timesteps\n",
    "    return 1  # Fallback to 1 if no valid timesteps found\n",
    "\n",
    "# Find suitable timesteps for vitals and labs\n",
    "timesteps_vitals = find_timesteps(X_vitals.shape[1], MAX_TIMESTEPS)\n",
    "timesteps_labs = find_timesteps(X_labs.shape[1], MAX_TIMESTEPS)\n",
    "\n",
    "# Calculate number of features per timestep\n",
    "num_features_vitals = X_vitals.shape[1] // timesteps_vitals\n",
    "num_features_labs = X_labs.shape[1] // timesteps_labs\n",
    "\n",
    "# Reshape data into 3D arrays (samples, timesteps, features)\n",
    "X_vitals = X_vitals.reshape((-1, timesteps_vitals, num_features_vitals))\n",
    "X_labs = X_labs.reshape((-1, timesteps_labs, num_features_labs))\n",
    "\n",
    "# Split data among clients\n",
    "def split_data(X_vitals, X_labs, y, num_clients):\n",
    "    vitals_splits = np.array_split(X_vitals, num_clients)\n",
    "    labs_splits = np.array_split(X_labs, num_clients)\n",
    "    labels_splits = np.array_split(y, num_clients)\n",
    "    return vitals_splits, labs_splits, labels_splits\n",
    "\n",
    "X_vitals_splits, X_labs_splits, y_splits = split_data(X_vitals, X_labs, y, CLIENT_COUNT)\n",
    "\n",
    "# Define input shapes\n",
    "vitals_spec = tf.TensorSpec(\n",
    "    shape=(timesteps_vitals, num_features_vitals),\n",
    "    dtype=tf.dtypes.float64,\n",
    "    name='vitals'\n",
    ")\n",
    "labs_spec = tf.TensorSpec(\n",
    "    shape=(timesteps_labs, num_features_labs),\n",
    "    dtype=tf.dtypes.float64,\n",
    "    name='labs'\n",
    ")\n",
    "\n",
    "# Define the custom F1-score metric\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.precision.result()\n",
    "        recall = self.recall.result()\n",
    "        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "# Define the model architecture\n",
    "def create_model():\n",
    "    # Vital channel\n",
    "    inputs_vitals = tf.keras.Input(shape=vitals_spec.shape, name='Input_vitals')\n",
    "    mask_vitals = tf.keras.layers.Masking(mask_value=-2., name='mask_vitals')(inputs_vitals)\n",
    "    GRU_layer1_vitals = tf.keras.layers.GRU(16, return_sequences=True, name='GRU_layer1_vitals')(mask_vitals)\n",
    "    GRU_layer2_vitals = tf.keras.layers.GRU(16, return_sequences=True, name='GRU_layer2_vitals')(GRU_layer1_vitals)\n",
    "    GRU_layer3_vitals = tf.keras.layers.GRU(16, return_sequences=False, name='GRU_layer3_vitals')(GRU_layer2_vitals)\n",
    "    normalized_vitals = tf.keras.layers.BatchNormalization(name='BatchNorm_vitals')(GRU_layer3_vitals)\n",
    "\n",
    "    # Labs channel\n",
    "    inputs_labs = tf.keras.Input(shape=labs_spec.shape, name='Input_labs')\n",
    "    mask_labs = tf.keras.layers.Masking(mask_value=-2., name='mask_labs')(inputs_labs)\n",
    "    GRU_layer1_labs = tf.keras.layers.GRU(16, return_sequences=True, name='GRU_layer1_labs')(mask_labs)\n",
    "    GRU_layer2_labs = tf.keras.layers.GRU(16, return_sequences=True, name    ='GRU_layer2_labs')(GRU_layer1_labs)\n",
    "    GRU_layer3_labs = tf.keras.layers.GRU(16, return_sequences=False, name='GRU_layer3_labs')(GRU_layer2_labs)\n",
    "    normalized_labs = tf.keras.layers.BatchNormalization(name='BatchNorm_labs')(GRU_layer3_labs)\n",
    "\n",
    "    # Concatenation of both branches\n",
    "    merge = tf.keras.layers.Concatenate()([normalized_vitals, normalized_labs])\n",
    "\n",
    "    FCL1 = tf.keras.layers.Dense(16, name='FCL1')(merge)\n",
    "    FCL2 = tf.keras.layers.Dense(16, name='FCL2')(FCL1)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(FCL2)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs_vitals, inputs_labs], outputs=outputs, name='RNN_model')\n",
    "    return model\n",
    "\n",
    "# Train model on each client's data and aggregate the weights\n",
    "def federated_training(X_vitals_splits, X_labs_splits, y_splits, num_clients, global_epochs, local_epochs, batch_size):\n",
    "    # Initialize the global model\n",
    "    global_model = create_model()\n",
    "    global_model.compile(optimizer='adam',\n",
    "                         loss='binary_crossentropy',\n",
    "                         metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), F1Score()])\n",
    "\n",
    "    for global_epoch in range(global_epochs):\n",
    "        print(f\"Global Epoch {global_epoch+1}/{global_epochs}\")\n",
    "\n",
    "        # Initialize list to store client models\n",
    "        client_models = []\n",
    "\n",
    "        for client in range(num_clients):\n",
    "            print(f\" Training on client {client+1}/{num_clients}\")\n",
    "            client_model = create_model()\n",
    "            client_model.compile(optimizer='adam',\n",
    "                                 loss='binary_crossentropy',\n",
    "                                 metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), F1Score()])\n",
    "            \n",
    "            # Set the client model weights to the global model weights\n",
    "            client_model.set_weights(global_model.get_weights())\n",
    "\n",
    "            # Train the client model\n",
    "            client_model.fit(x=[X_vitals_splits[client], X_labs_splits[client]],\n",
    "                             y=y_splits[client],\n",
    "                             epochs=local_epochs,\n",
    "                             batch_size=batch_size,\n",
    "                             verbose=0)\n",
    "            \n",
    "            # Append trained client model to the list\n",
    "            client_models.append(client_model)\n",
    "\n",
    "        # Aggregate the client model weights\n",
    "        average_weights = []\n",
    "        for weights_list in zip(*[client.get_weights() for client in client_models]):\n",
    "            average_weights.append(np.mean(weights_list, axis=0))\n",
    "\n",
    "        # Update global model with the aggregated weights\n",
    "        global_model.set_weights(average_weights)\n",
    "\n",
    "    return global_model\n",
    "\n",
    "# Perform federated training\n",
    "global_model = federated_training(X_vitals_splits, X_labs_splits, y_splits, CLIENT_COUNT, global_epochs=3, local_epochs=5, batch_size=32)\n",
    "\n",
    "# Evaluate the global model\n",
    "evaluation = global_model.evaluate(x=[X_vitals, X_labs], y=y, verbose=0)\n",
    "\n",
    "# Print out the evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {evaluation[1]}\")\n",
    "print(f\"Precision: {evaluation[2]}\")\n",
    "print(f\"Recall: {evaluation[3]}\")\n",
    "print(f\"F1 Score: {evaluation[4]}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
